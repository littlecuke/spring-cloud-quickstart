## Elasticsearch 安装

### 1.部署单点es

#### 1.1 创建网络

因为后面还需要部署Kibana容器，所以需要让es和Kibana容器互联

```bash
docker network create es-net
```

#### 1.2 拉取镜像

- elasticsearch
```bash
docker pull elasticsearch:7.12.1
```

- kibana

```bash
docker pull kibana:7.12.1
```

#### 1.3 运行

```bash
docker run \
  --name es \
  --network es-net \
  -p 9200:9200 \
  -p 9300:9300 \
  -v es-data:/usr/share/elasticsearch/data \
  -v es-plugins:/usr/share/elasticsearch/plugins \
  -e "discovery.type=single-node" \
  -d \
  elasticsearch:7.12.1
```

在浏览器中输入：http://localhost:9200/，即可查看到es的响应结果，localhost对应服务器的ip

### 2.部署kibana

```bash
docker run \
  --name kibana \
  --network=es-net \
  -p 5601:5601 \
  -e ELASTICSEARCH_HOSTS=http://es:9200 \
  -d \
  kibana:7.12.1
```

在浏览器中输入：http://localhost:5601/，对kibana进行查看

### 3.分词器

es在创建倒排索引的时候需要对文档进行分词，在搜索时，需要对用户输入的内容进行分词，但是默认的分词规则对中文处理并不友好，
因此我们需要安装IK分词器。

#### 3.1 查看es插件目录

```bash
docker volume inspect es-plugins
```

得到的目录：`/var/lib/docker/volumes/es-plugins/_data`

### 3.2 下载ik分词器

ik分词器的下载链接：`https://github.com/medcl/elasticsearch-analysis-ik/releases?page=2`

将下载的压缩包进行解压，并上传至es的插件目录中

### 3.3 重启容器

```bash
docker restart es
```

### 3.4 测试

IK分词器包含两种模式：

- ik_smart：最少切分
- ik_max_word：最细切分

```http request
POST /_analyze
{
    "text": "测试分词器安装是否成功",
    "analyzer": "ik_smart"
}
```

测试结果：

```json
{
  "tokens" : [
    {
      "token" : "测试",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "CN_WORD",
      "position" : 0
    },
    {
      "token" : "分词器",
      "start_offset" : 2,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 1
    },
    {
      "token" : "安装",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "CN_WORD",
      "position" : 2
    },
    {
      "token" : "是否",
      "start_offset" : 7,
      "end_offset" : 9,
      "type" : "CN_WORD",
      "position" : 3
    },
    {
      "token" : "成功",
      "start_offset" : 9,
      "end_offset" : 11,
      "type" : "CN_WORD",
      "position" : 4
    }
  ]
}

```